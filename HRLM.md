# Hierarchical Reasoning Language Model (HRLM)

## DocumentaciÃ³n para AdaptaciÃ³n de HRM a Procesamiento de Lenguaje Natural

---

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [AnÃ¡lisis de la Arquitectura HRM Original](#anÃ¡lisis-de-la-arquitectura-hrm-original)
3. [Adaptaciones Necesarias para Texto](#adaptaciones-necesarias-para-texto)
4. [Estrategias de TokenizaciÃ³n](#estrategias-de-tokenizaciÃ³n)
5. [Modificaciones Arquitecturales](#modificaciones-arquitecturales)
6. [Formato de Datos y Dataset](#formato-de-datos-y-dataset)
7. [Sistema de Entrenamiento](#sistema-de-entrenamiento)
8. [DecodificaciÃ³n y GeneraciÃ³n](#decodificaciÃ³n-y-generaciÃ³n)
9. [ImplementaciÃ³n PrÃ¡ctica](#implementaciÃ³n-prÃ¡ctica)
10. [Casos de Uso y Aplicaciones](#casos-de-uso-y-aplicaciones)
11. [DesafÃ­os y Limitaciones](#desafÃ­os-y-limitaciones)
12. [GuÃ­a de ImplementaciÃ³n](#guÃ­a-de-implementaciÃ³n)
13. [ConfiguraciÃ³n Recomendada](#configuraciÃ³n-recomendada)
14. [Roadmap de Desarrollo](#roadmap-de-desarrollo)

---

## ğŸ¯ IntroducciÃ³n

El **Hierarchical Reasoning Language Model (HRLM)** es una adaptaciÃ³n del modelo HRM (Hierarchical Reasoning Model) para el procesamiento de lenguaje natural. Esta adaptaciÃ³n mantiene las fortalezas Ãºnicas de HRM mientras extiende su aplicabilidad a tareas de texto que requieren razonamiento estructurado.

### MotivaciÃ³n

HRM ha demostrado capacidades excepcionales en:
- âœ… **Eficiencia de datos**: Aprendizaje con solo 1000 ejemplos
- âœ… **Razonamiento jerÃ¡rquico**: Procesamiento en mÃºltiples niveles de abstracciÃ³n
- âœ… **ComputaciÃ³n adaptativa**: Tiempo de procesamiento variable segÃºn complejidad
- âœ… **Estabilidad de entrenamiento**: Arquitectura recurrente estable

HRLM busca trasladar estas ventajas al dominio del procesamiento de lenguaje natural.

---

## ğŸ—ï¸ AnÃ¡lisis de la Arquitectura HRM Original

### Componentes Principales

#### 1. **Arquitectura JerÃ¡rquica Dual**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   H-Level       â”‚â—„â”€â”€â–ºâ”‚   L-Level       â”‚
â”‚ (PlanificaciÃ³n  â”‚    â”‚ (EjecuciÃ³n      â”‚
â”‚  Abstracta)     â”‚    â”‚  Detallada)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **H-Level**: MÃ³dulo de alto nivel para planificaciÃ³n lenta y abstracta
- **L-Level**: MÃ³dulo de bajo nivel para computaciones rÃ¡pidas y detalladas

#### 2. **Procesamiento Recurrente**
```python
# Ciclos de procesamiento
for h_cycle in range(H_cycles):
    for l_cycle in range(L_cycles):
        if not (last_h_cycle and last_l_cycle):
            z_L = L_level(z_L, z_H + input_embeddings)
    if not last_h_cycle:
        z_H = H_level(z_H, z_L)

# Solo el Ãºltimo paso tiene gradientes
z_L = L_level(z_L, z_H + input_embeddings)  # Con gradientes
z_H = H_level(z_H, z_L)                     # Con gradientes
```

#### 3. **Mecanismo ACT (Adaptive Computation Time)**
- **Q-heads**: Predicen cuÃ¡ndo parar el procesamiento
- **ExploraciÃ³n**: Probabilidad configurable durante entrenamiento
- **Pasos mÃ¡ximos**: LÃ­mite superior de iteraciones

#### 4. **ConfiguraciÃ³n Actual**
```yaml
# ConfiguraciÃ³n tÃ­pica para Sudoku
vocab_size: 11        # PAD + dÃ­gitos 0-9
seq_len: 81          # Grilla 9x9 aplanada
hidden_size: 512     # DimensiÃ³n de embeddings
H_cycles: 2          # Ciclos de alto nivel
L_cycles: 2          # Ciclos de bajo nivel
halt_max_steps: 16   # Pasos mÃ¡ximos ACT
```

---

## ğŸ”„ Adaptaciones Necesarias para Texto

### Diferencias Fundamentales

| Aspecto | HRM Original | HRLM (Texto) |
|---------|--------------|--------------|
| **Vocabulario** | 11-12 tokens | 1000-50000 tokens |
| **Secuencias** | 81 tokens (fijo) | 512-2048 tokens (variable) |
| **Dominio** | Problemas determinÃ­sticos | Lenguaje natural |
| **Salidas** | SoluciÃ³n Ãºnica | MÃºltiples respuestas vÃ¡lidas |
| **EvaluaciÃ³n** | Exactitud objetiva | MÃ©tricas de calidad textual |

### DesafÃ­os Principales

1. **Escalabilidad de Vocabulario**: Embeddings y heads de salida mucho mÃ¡s grandes
2. **Longitud Variable**: Secuencias de texto de longitud variable vs. grillas fijas
3. **Complejidad SemÃ¡ntica**: Significado contextual vs. valores numÃ©ricos
4. **GeneraciÃ³n Coherente**: Mantener coherencia en secuencias largas

---

## ğŸ”¤ Estrategias de TokenizaciÃ³n

### OpciÃ³n 1: TokenizaciÃ³n a Nivel de CarÃ¡cter (Recomendada)

```python
class CharTokenizer:
    def __init__(self):
        # Conjunto bÃ¡sico de caracteres
        self.charset = "abcdefghijklmnopqrstuvwxyz" + \
                      "ABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
                      "0123456789 .,!?;:-()[]{}'\"\n"
        
        # Tokens especiales
        self.PAD_TOKEN = 0
        self.EOS_TOKEN = 1
        self.UNK_TOKEN = 2
        self.SEP_TOKEN = 3  # Separador entrada/salida
        
        # Crear mapeos
        self.char_to_id = {char: idx + 4 for idx, char in enumerate(self.charset)}
        self.vocab_size = len(self.charset) + 4
```

**Ventajas:**
- âœ… Vocabulario pequeÃ±o (~100 tokens)
- âœ… Compatible con arquitectura HRM actual
- âœ… No requiere tokenizadores externos
- âœ… Maneja cualquier texto sin tokens OOV

**Desventajas:**
- âŒ Secuencias mÃ¡s largas
- âŒ Menos eficiente para palabras comunes

### OpciÃ³n 2: TokenizaciÃ³n HÃ­brida

```python
class HybridTokenizer:
    def __init__(self):
        # Palabras mÃ¡s comunes (top 500-1000)
        self.common_words = ["the", "and", "is", "to", "in", ...]
        
        # Caracteres para palabras no comunes
        self.charset = "abcdefghijklmnopqrstuvwxyz0123456789 .,!?"
        
        # Tokens especiales
        self.special_tokens = ["<PAD>", "<EOS>", "<UNK>", "<SEP>"]
        
        self.vocab_size = len(self.common_words) + len(self.charset) + len(self.special_tokens)
```

**Ventajas:**
- âœ… Eficiencia para palabras comunes
- âœ… Vocabulario manejable (~1000 tokens)
- âœ… Flexibilidad para palabras raras

### OpciÃ³n 3: BPE/WordPiece (Avanzada)

```python
# Usar tokenizadores estÃ¡ndar (GPT, BERT)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
vocab_size = tokenizer.vocab_size  # ~50K tokens
```

**Ventajas:**
- âœ… MÃ¡s eficiente para texto largo
- âœ… Mejor representaciÃ³n semÃ¡ntica

**Desventajas:**
- âŒ Vocabulario muy grande
- âŒ Requiere modificaciones significativas en HRM

---

## ğŸ›ï¸ Modificaciones Arquitecturales

### ConfiguraciÃ³n Adaptada para Texto

```python
class TextHRMConfig:
    # ConfiguraciÃ³n de texto
    vocab_size: int = 1000        # Vocabulario expandido
    seq_len: int = 1024          # Secuencias mÃ¡s largas
    max_input_length: int = 512   # Longitud mÃ¡xima de entrada
    max_output_length: int = 512  # Longitud mÃ¡xima de salida
    
    # HRM expandido para texto
    hidden_size: int = 768        # vs 512 original
    H_cycles: int = 4             # vs 2 original (mÃ¡s razonamiento)
    L_cycles: int = 4             # vs 2 original
    H_layers: int = 6             # vs 4 original (mÃ¡s capacidad)
    L_layers: int = 6             # vs 4 original
    num_heads: int = 12           # vs 8 original
    halt_max_steps: int = 64      # vs 16 original (mÃ¡s pasos)
    
    # Nuevos parÃ¡metros
    use_input_output_separation: bool = True
    input_output_separator_token: int = 3
```

### Modificaciones en Embeddings

```python
# Embeddings escalados para vocabulario mayor
self.embed_tokens = CastedEmbedding(
    num_embeddings=config.vocab_size,    # 1000+ vs 11
    embedding_dim=config.hidden_size,    # 768 vs 512
    init_std=1.0 / math.sqrt(config.hidden_size),
    cast_to=config.forward_dtype
)

# Head de salida escalado
self.lm_head = CastedLinear(
    in_features=config.hidden_size,      # 768 vs 512
    out_features=config.vocab_size,      # 1000+ vs 11
    bias=False
)
```

### Procesamiento de Secuencias Largas

```python
# Ajustes para secuencias mÃ¡s largas
if config.pos_encodings == "rope":
    self.rotary_emb = RotaryEmbedding(
        dim=config.hidden_size // config.num_heads,
        max_position_embeddings=config.seq_len,  # 1024 vs 81
        base=config.rope_theta
    )
```

---

## ğŸ“Š Formato de Datos y Dataset

### Estructura de Datos para Texto

```python
# Formato de secuencia: [INPUT] [SEP] [OUTPUT] [EOS]
# Ejemplo: "Translate: Hello" [SEP] "Hola" [EOS]

def build_text_dataset(input_texts, output_texts, tokenizer, max_seq_len=1024):
    results = {
        "inputs": [],      # Secuencia completa
        "labels": [],      # Solo salida (entrada enmascarada)
        "puzzle_identifiers": [],
        "puzzle_indices": [],
        "group_indices": []
    }
    
    for inp_text, out_text in zip(input_texts, output_texts):
        # Tokenizar
        inp_tokens = tokenizer.encode(inp_text)
        out_tokens = tokenizer.encode(out_text)
        
        # Crear secuencia completa
        full_sequence = inp_tokens + [SEP_TOKEN] + out_tokens + [EOS_TOKEN]
        
        # Pad a longitud fija
        padded_sequence = full_sequence + [PAD_TOKEN] * (max_seq_len - len(full_sequence))
        
        # Input: secuencia completa
        inputs = np.array(padded_sequence[:max_seq_len])
        
        # Labels: -100 para entrada, tokens reales para salida
        labels = np.full(max_seq_len, -100)  # -100 = ignore
        sep_pos = np.where(inputs == SEP_TOKEN)[0]
        if len(sep_pos) > 0:
            start_output = sep_pos[0] + 1
            output_length = min(len(out_tokens), max_seq_len - start_output)
            labels[start_output:start_output + output_length] = out_tokens[:output_length]
        
        results["inputs"].append(inputs)
        results["labels"].append(labels)
        results["puzzle_identifiers"].append(0)
```

### Metadata para Texto

```python
metadata = PuzzleDatasetMetadata(
    seq_len=max_seq_len,
    vocab_size=tokenizer.vocab_size,
    pad_id=PAD_TOKEN,
    ignore_label_id=-100,
    blank_identifier_id=0,
    num_puzzle_identifiers=1,
    total_groups=len(input_texts),
    mean_puzzle_examples=1,
    sets=["train", "test"]
)
```

---

## ğŸ¯ Sistema de Entrenamiento

### FunciÃ³n de PÃ©rdida Adaptada

```python
def text_aware_loss(logits, labels, ignore_index=-100):
    """
    Calcula pÃ©rdida solo en tokens de salida
    """
    # Flatten para cross entropy
    flat_logits = logits.view(-1, logits.size(-1))
    flat_labels = labels.view(-1)
    
    # Cross entropy con ignore_index
    loss = F.cross_entropy(
        flat_logits, 
        flat_labels, 
        ignore_index=ignore_index, 
        reduction='none'
    )
    
    # Promediar por secuencia vÃ¡lida
    loss = loss.view(labels.shape)
    valid_mask = (labels != ignore_index)
    seq_losses = loss.sum(dim=1) / valid_mask.sum(dim=1).clamp(min=1)
    
    return seq_losses.mean()
```

### ConfiguraciÃ³n de Entrenamiento

```yaml
# ConfiguraciÃ³n adaptada para texto
data_path: "data/text-dataset"

# HiperparÃ¡metros ajustados
global_batch_size: 32         # Menor por memoria (vs 768)
epochs: 100000               # MÃ¡s Ã©pocas para convergencia
eval_interval: 5000          # EvaluaciÃ³n mÃ¡s frecuente

lr: 1e-5                     # Learning rate mÃ¡s conservador
lr_min_ratio: 0.1
lr_warmup_steps: 10000       # MÃ¡s warmup para estabilidad

weight_decay: 0.01           # Menos regularizaciÃ³n
beta1: 0.9
beta2: 0.95

# Sin puzzle embeddings para texto bÃ¡sico
puzzle_emb_lr: 0
puzzle_emb_weight_decay: 0
```

### MÃ©tricas de EvaluaciÃ³n

```python
def evaluate_text_quality(predictions, references):
    """
    MÃ©tricas especÃ­ficas para texto
    """
    metrics = {}
    
    # Exactitud a nivel de secuencia
    exact_matches = sum(p == r for p, r in zip(predictions, references))
    metrics["exact_accuracy"] = exact_matches / len(predictions)
    
    # BLEU score para calidad de generaciÃ³n
    from nltk.translate.bleu_score import sentence_bleu
    bleu_scores = [
        sentence_bleu([ref.split()], pred.split()) 
        for pred, ref in zip(predictions, references)
    ]
    metrics["bleu_score"] = np.mean(bleu_scores)
    
    # Longitud promedio generada
    metrics["avg_length"] = np.mean([len(p.split()) for p in predictions])
    
    return metrics
```

---

## ğŸ” DecodificaciÃ³n y GeneraciÃ³n

### DecodificaciÃ³n No-Autorregresiva (Recomendada)

```python
def decode_text_output(model_output, tokenizer, input_sequence, separator_token=3):
    """
    Decodifica salida del modelo manteniendo arquitectura HRM
    """
    # model_output: [batch, seq_len, vocab_size]
    predicted_tokens = torch.argmax(model_output, dim=-1)
    
    decoded_texts = []
    for batch_idx in range(predicted_tokens.shape[0]):
        pred_seq = predicted_tokens[batch_idx]
        inp_seq = input_sequence[batch_idx]
        
        # Encontrar separador
        sep_positions = torch.where(inp_seq == separator_token)[0]
        
        if len(sep_positions) > 0:
            # Extraer solo parte de salida
            start_output = sep_positions[0] + 1
            output_tokens = pred_seq[start_output:]
            
            # Encontrar EOS
            eos_positions = torch.where(output_tokens == tokenizer.EOS_TOKEN)[0]
            if len(eos_positions) > 0:
                output_tokens = output_tokens[:eos_positions[0]]
            
            decoded_text = tokenizer.decode(output_tokens.tolist())
        else:
            decoded_text = tokenizer.decode(pred_seq.tolist())
        
        decoded_texts.append(decoded_text)
    
    return decoded_texts
```

### Pipeline de Inferencia

```python
class HRLMInference:
    def __init__(self, model, tokenizer, config):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
    
    def predict(self, input_text):
        """
        Predice texto de salida para entrada dada
        """
        # Preparar entrada
        input_tokens = self.tokenizer.encode(input_text)
        input_tensor = torch.tensor([input_tokens + [self.tokenizer.SEP_TOKEN]], 
                                   dtype=torch.int32)
        
        # Pad a longitud del modelo
        padded_input = F.pad(input_tensor, 
                           (0, self.config.seq_len - input_tensor.size(1)), 
                           value=self.tokenizer.PAD_TOKEN)
        
        batch = {
            "inputs": padded_input,
            "puzzle_identifiers": torch.zeros(1, dtype=torch.int32)
        }
        
        # Inferencia con ACT
        carry = self.model.initial_carry(batch)
        
        while True:
            carry, outputs = self.model(carry, batch)
            if outputs["halted"].all():
                break
        
        # Decodificar salida
        decoded = decode_text_output(
            outputs["logits"], 
            self.tokenizer, 
            padded_input,
            self.tokenizer.SEP_TOKEN
        )
        
        return decoded[0]
```

---

## ğŸ’» ImplementaciÃ³n PrÃ¡ctica

### Estructura de Archivos

```
HRLM/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hrm/
â”‚   â”‚   â”œâ”€â”€ hrm_text_v1.py          # Modelo adaptado para texto
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizers/
â”‚   â”‚   â”œâ”€â”€ char_tokenizer.py       # Tokenizador de caracteres
â”‚   â”‚   â”œâ”€â”€ hybrid_tokenizer.py     # Tokenizador hÃ­brido
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ layers.py                   # Capas reutilizadas de HRM
â”‚   â”œâ”€â”€ losses.py                   # PÃ©rdidas adaptadas
â”‚   â””â”€â”€ common.py
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ build_text_dataset.py       # Constructor de datasets de texto
â”‚   â”œâ”€â”€ text_dataset.py             # Dataset loader para texto
â”‚   â””â”€â”€ common.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ arch/
â”‚   â”‚   â””â”€â”€ hrlm_v1.yaml           # ConfiguraciÃ³n HRLM
â”‚   â””â”€â”€ cfg_text_pretrain.yaml     # ConfiguraciÃ³n de entrenamiento
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_text.py              # Script de entrenamiento
â”‚   â”œâ”€â”€ evaluate_text.py           # Script de evaluaciÃ³n
â”‚   â””â”€â”€ inference_text.py          # Script de inferencia
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ translation/               # Ejemplo de traducciÃ³n
â”‚   â”œâ”€â”€ qa/                        # Ejemplo de Q&A
â”‚   â””â”€â”€ summarization/             # Ejemplo de resumen
â””â”€â”€ HRLM.md                        # Esta documentaciÃ³n
```

### Tokenizador Base

```python
# models/tokenizers/char_tokenizer.py
class CharTokenizer:
    def __init__(self, charset=None):
        if charset is None:
            self.charset = (
                "abcdefghijklmnopqrstuvwxyz"
                "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                "0123456789 .,!?;:-()[]{}'\"\n\t"
            )
        else:
            self.charset = charset
        
        # Tokens especiales
        self.special_tokens = {
            "<PAD>": 0,
            "<EOS>": 1, 
            "<UNK>": 2,
            "<SEP>": 3
        }
        
        # Crear mapeos
        self.char_to_id = self.special_tokens.copy()
        for i, char in enumerate(self.charset):
            self.char_to_id[char] = i + len(self.special_tokens)
        
        self.id_to_char = {v: k for k, v in self.char_to_id.items()}
        self.vocab_size = len(self.char_to_id)
    
    @property
    def PAD_TOKEN(self): return 0
    @property  
    def EOS_TOKEN(self): return 1
    @property
    def UNK_TOKEN(self): return 2
    @property
    def SEP_TOKEN(self): return 3
    
    def encode(self, text, add_eos=True):
        tokens = []
        for char in text:
            tokens.append(self.char_to_id.get(char, self.UNK_TOKEN))
        if add_eos:
            tokens.append(self.EOS_TOKEN)
        return tokens
    
    def decode(self, tokens):
        chars = []
        for token in tokens:
            if token == self.EOS_TOKEN:
                break
            elif token == self.PAD_TOKEN:
                continue
            elif token in self.id_to_char:
                chars.append(self.id_to_char[token])
        return ''.join(chars)
```

### Constructor de Dataset

```python
# dataset/build_text_dataset.py
def build_text_dataset(
    input_texts: List[str],
    output_texts: List[str], 
    tokenizer,
    output_dir: str,
    max_seq_len: int = 1024,
    train_split: float = 0.8
):
    """
    Construye dataset de texto compatible con HRM
    """
    assert len(input_texts) == len(output_texts)
    
    # Dividir en train/test
    n_train = int(len(input_texts) * train_split)
    
    splits = {
        "train": (input_texts[:n_train], output_texts[:n_train]),
        "test": (input_texts[n_train:], output_texts[n_train:])
    }
    
    for split_name, (inputs, outputs) in splits.items():
        results = process_text_split(inputs, outputs, tokenizer, max_seq_len)
        
        # Guardar split
        split_dir = os.path.join(output_dir, split_name)
        os.makedirs(split_dir, exist_ok=True)
        
        for key, data in results.items():
            np.save(os.path.join(split_dir, f"{key}.npy"), data)
        
        # Metadata
        metadata = PuzzleDatasetMetadata(
            seq_len=max_seq_len,
            vocab_size=tokenizer.vocab_size,
            pad_id=tokenizer.PAD_TOKEN,
            ignore_label_id=-100,
            blank_identifier_id=0,
            num_puzzle_identifiers=1,
            total_groups=len(inputs),
            mean_puzzle_examples=1,
            sets=[split_name]
        )
        
        with open(os.path.join(split_dir, "metadata.json"), "w") as f:
            json.dump(metadata.model_dump(), f, indent=2)
    
    print(f"Dataset guardado en {output_dir}")
    return metadata
```

---

## ğŸ® Casos de Uso y Aplicaciones

### Nivel 1: Tareas Estructuradas (MÃ¡s Adecuadas)

#### 1.1 TraducciÃ³n Simple
```python
# Entrada: "Translate to Spanish: Hello world"
# Salida: "Hola mundo"

input_texts = [
    "Translate to Spanish: Hello world",
    "Translate to Spanish: How are you?",
    "Translate to Spanish: Good morning"
]
output_texts = [
    "Hola mundo",
    "Â¿CÃ³mo estÃ¡s?", 
    "Buenos dÃ­as"
]
```

#### 1.2 TransformaciÃ³n de Formato
```python
# JSON a texto natural
input_texts = [
    'Convert to sentence: {"name": "John", "age": 25}',
    'Convert to sentence: {"city": "Madrid", "country": "Spain"}'
]
output_texts = [
    "John is 25 years old",
    "Madrid is in Spain"
]
```

#### 1.3 Respuesta a Preguntas Factuales
```python
input_texts = [
    "Question: What is the capital of France?",
    "Question: What is 2 + 2?",
    "Question: What color is the sky?"
]
output_texts = [
    "Paris",
    "4", 
    "Blue"
]
```

### Nivel 2: Tareas de Razonamiento (Moderadamente Adecuadas)

#### 2.1 CorrecciÃ³n Gramatical
```python
input_texts = [
    "Fix grammar: I are going to school",
    "Fix grammar: She don't like apples"
]
output_texts = [
    "I am going to school",
    "She doesn't like apples"
]
```

#### 2.2 ExtracciÃ³n de InformaciÃ³n
```python
input_texts = [
    "Extract name: My name is John Smith and I live in Madrid",
    "Extract date: The meeting is on March 15, 2024"
]
output_texts = [
    "John Smith",
    "March 15, 2024"
]
```

### Nivel 3: Tareas Creativas (Menos Adecuadas)

#### 3.1 Resumen Extractivo
```python
input_texts = [
    "Summarize: The weather today is very nice and sunny. Perfect for outdoor activities."
]
output_texts = [
    "Nice sunny weather, good for outdoors"
]
```

---

## âš ï¸ DesafÃ­os y Limitaciones

### DesafÃ­os TÃ©cnicos

#### 1. **Escalabilidad de Memoria**
```python
# Problema: Vocabulario grande aumenta memoria exponencialmente
# Sudoku: vocab_size=11, hidden_size=512
# Embedding: 11 * 512 = 5.6K parÃ¡metros
# LM Head: 512 * 11 = 5.6K parÃ¡metros

# Texto: vocab_size=10000, hidden_size=768  
# Embedding: 10000 * 768 = 7.68M parÃ¡metros
# LM Head: 768 * 10000 = 7.68M parÃ¡metros
# Total: ~15M parÃ¡metros adicionales solo en embeddings
```

#### 2. **Longitud de Secuencia**
```python
# AtenciÃ³n cuadrÃ¡tica: O(nÂ²)
# Sudoku: 81Â² = 6,561 operaciones
# Texto: 1024Â² = 1,048,576 operaciones (~160x mÃ¡s)
```

#### 3. **Convergencia de Entrenamiento**
```python
# HRM converge rÃ¡pido en Sudoku (pocos parÃ¡metros, datos estructurados)
# Texto requiere:
# - MÃ¡s Ã©pocas de entrenamiento
# - Learning rates mÃ¡s pequeÃ±os
# - MÃ¡s datos para generalizaciÃ³n
```

### Limitaciones Conceptuales

#### 1. **Naturaleza No-Autorregresiva**
- âœ… **Ventaja**: GeneraciÃ³n paralela, mÃ¡s rÃ¡pida
- âŒ **LimitaciÃ³n**: DifÃ­cil mantener coherencia en texto largo
- âŒ **LimitaciÃ³n**: No puede corregir errores anteriores

#### 2. **Razonamiento JerÃ¡rquico en Texto**
```python
# HRM asume separaciÃ³n clara:
# H-Level: PlanificaciÃ³n abstracta
# L-Level: EjecuciÃ³n detallada

# En texto, esta separaciÃ³n puede no ser natural:
# - Sintaxis y semÃ¡ntica estÃ¡n entrelazadas
# - Decisiones locales afectan estructura global
# - Diferentes tipos de texto requieren diferentes patrones
```

#### 3. **EvaluaciÃ³n de Calidad**
```python
# Sudoku: SoluciÃ³n correcta o incorrecta (binario)
# Texto: MÃºltiples respuestas vÃ¡lidas, calidad subjetiva

# MÃ©tricas necesarias:
# - BLEU, ROUGE para calidad
# - Coherencia semÃ¡ntica  
# - Fluidez y naturalidad
# - Relevancia contextual
```

### Limitaciones de Datos

#### 1. **Eficiencia de Datos**
- HRM aprende Sudoku con 1000 ejemplos
- Texto puede requerir 10K-100K ejemplos para generalizar
- Vocabulario grande dificulta few-shot learning

#### 2. **Diversidad LingÃ¼Ã­stica**
- Modelos de caracteres luchan con idiomas no latinos
- TokenizaciÃ³n hÃ­brida requiere vocabularios especÃ­ficos por idioma
- Transferencia entre idiomas limitada

---

## ğŸš€ GuÃ­a de ImplementaciÃ³n

### Fase 1: PreparaciÃ³n del Entorno

#### 1.1 InstalaciÃ³n de Dependencias
```bash
# Clonar repositorio HRM base
git clone https://github.com/sapientinc/HRM.git
cd HRM

# Instalar dependencias base
pip install -r requirements.txt

# Dependencias adicionales para texto
pip install nltk transformers datasets
```

#### 1.2 Estructura de Proyecto
```bash
# Crear estructura para HRLM
mkdir -p models/tokenizers
mkdir -p dataset/text
mkdir -p config/text
mkdir -p examples/text
mkdir -p scripts/text
```

### Fase 2: ImplementaciÃ³n del Tokenizador

#### 2.1 Tokenizador de Caracteres
```python
# Crear models/tokenizers/char_tokenizer.py
# (Ver implementaciÃ³n completa en secciÃ³n anterior)

# Probar tokenizador
python -c "
from models.tokenizers.char_tokenizer import CharTokenizer
tokenizer = CharTokenizer()
text = 'Hello world!'
encoded = tokenizer.encode(text)
decoded = tokenizer.decode(encoded)
print(f'Original: {text}')
print(f'Encoded: {encoded}')
print(f'Decoded: {decoded}')
print(f'Vocab size: {tokenizer.vocab_size}')
"
```

### Fase 3: ConstrucciÃ³n del Dataset

#### 3.1 Dataset de Ejemplo
```python
# Crear dataset/text/build_simple_dataset.py
from models.tokenizers.char_tokenizer import CharTokenizer
from dataset.build_text_dataset import build_text_dataset

# Datos de ejemplo para traducciÃ³n
input_texts = [
    "Translate to Spanish: Hello",
    "Translate to Spanish: Goodbye", 
    "Translate to Spanish: Thank you",
    "Translate to Spanish: Good morning",
    "Translate to Spanish: How are you?"
] * 200  # Repetir para tener mÃ¡s datos

output_texts = [
    "Hola",
    "AdiÃ³s",
    "Gracias", 
    "Buenos dÃ­as",
    "Â¿CÃ³mo estÃ¡s?"
] * 200

# Construir dataset
tokenizer = CharTokenizer()
metadata = build_text_dataset(
    input_texts,
    output_texts,
    tokenizer,
    output_dir="data/translation-simple",
    max_seq_len=256
)
```

#### 3.2 Verificar Dataset
```python
# Verificar que el dataset se construyÃ³ correctamente
python -c "
import numpy as np
import json

# Cargar datos
inputs = np.load('data/translation-simple/train/inputs.npy')
labels = np.load('data/translation-simple/train/labels.npy')

with open('data/translation-simple/train/metadata.json') as f:
    metadata = json.load(f)

print(f'Inputs shape: {inputs.shape}')
print(f'Labels shape: {labels.shape}')
print(f'Vocab size: {metadata[\"vocab_size\"]}')
print(f'Seq len: {metadata[\"seq_len\"]}')
"
```

### Fase 4: ConfiguraciÃ³n del Modelo

#### 4.1 ConfiguraciÃ³n HRLM
```yaml
# Crear config/text/hrlm_v1.yaml
name: hrm.hrm_act_v1@HierarchicalReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy

# ConfiguraciÃ³n adaptada para texto
halt_exploration_prob: 0.1
halt_max_steps: 32

H_cycles: 4
L_cycles: 4

H_layers: 6
L_layers: 6

hidden_size: 768
num_heads: 12
expansion: 4

puzzle_emb_ndim: 0  # Sin puzzle embeddings para texto
pos_encodings: rope
```

#### 4.2 ConfiguraciÃ³n de Entrenamiento
```yaml
# Crear config/text/cfg_text_pretrain.yaml
defaults:
  - arch: text/hrlm_v1
  - _self_

# Datos
data_path: data/translation-simple

# HiperparÃ¡metros adaptados para texto
global_batch_size: 32
epochs: 50000
eval_interval: 5000

lr: 1e-5
lr_min_ratio: 0.1
lr_warmup_steps: 10000

weight_decay: 0.01
beta1: 0.9
beta2: 0.95

# Sin puzzle embeddings
puzzle_emb_lr: 0
puzzle_emb_weight_decay: 0
```

### Fase 5: Entrenamiento

#### 5.1 Script de Entrenamiento
```python
# Crear scripts/text/train_text.py
import sys
sys.path.append('.')

from pretrain import launch
import hydra
from omegaconf import DictConfig

@hydra.main(config_path="../../config/text", config_name="cfg_text_pretrain", version_base=None)
def main(cfg: DictConfig):
    launch(cfg)

if __name__ == "__main__":
    main()
```

#### 5.2 Ejecutar Entrenamiento
```bash
# Entrenamiento en GPU Ãºnica
cd scripts/text
python train_text.py

# Entrenamiento distribuido (mÃºltiples GPUs)
OMP_NUM_THREADS=8 torchrun --nproc-per-node 4 train_text.py
```

### Fase 6: EvaluaciÃ³n e Inferencia

#### 6.1 Script de EvaluaciÃ³n
```python
# Crear scripts/text/evaluate_text.py
from models.tokenizers.char_tokenizer import CharTokenizer
from evaluate import launch as evaluate_launch

def evaluate_text_model(checkpoint_path):
    # Cargar tokenizador
    tokenizer = CharTokenizer()
    
    # Evaluar modelo
    results = evaluate_launch(checkpoint=checkpoint_path)
    
    return results
```

#### 6.2 Script de Inferencia
```python
# Crear scripts/text/inference_text.py
import torch
from models.tokenizers.char_tokenizer import CharTokenizer

class HRLMInference:
    def __init__(self, checkpoint_path):
        self.tokenizer = CharTokenizer()
        # Cargar modelo (implementar carga desde checkpoint)
        
    def predict(self, input_text):
        # Implementar inferencia
        # (Ver implementaciÃ³n completa en secciÃ³n anterior)
        pass

# Ejemplo de uso
if __name__ == "__main__":
    model = HRLMInference("checkpoints/hrlm/step_50000")
    
    result = model.predict("Translate to Spanish: Hello world")
    print(f"Input: Translate to Spanish: Hello world")
    print(f"Output: {result}")
```

---

## âš™ï¸ ConfiguraciÃ³n Recomendada

### ConfiguraciÃ³n Base (Desarrollo)

```yaml
# Para experimentaciÃ³n inicial
arch:
  hidden_size: 512
  H_cycles: 2
  L_cycles: 2
  H_layers: 4
  L_layers: 4
  num_heads: 8
  halt_max_steps: 16

training:
  global_batch_size: 16
  lr: 5e-5
  epochs: 10000
  
data:
  vocab_size: 100    # Tokenizador de caracteres bÃ¡sico
  seq_len: 256       # Secuencias cortas
```

### ConfiguraciÃ³n Intermedia (ProducciÃ³n PequeÃ±a)

```yaml
# Para tareas reales pequeÃ±as
arch:
  hidden_size: 768
  H_cycles: 3
  L_cycles: 3  
  H_layers: 6
  L_layers: 6
  num_heads: 12
  halt_max_steps: 32

training:
  global_batch_size: 32
  lr: 1e-5
  epochs: 50000

data:
  vocab_size: 1000   # Tokenizador hÃ­brido
  seq_len: 512       # Secuencias medianas
```

### ConfiguraciÃ³n Avanzada (ProducciÃ³n Grande)

```yaml
# Para tareas complejas
arch:
  hidden_size: 1024
  H_cycles: 4
  L_cycles: 4
  H_layers: 8
  L_layers: 8
  num_heads: 16
  halt_max_steps: 64

training:
  global_batch_size: 64
  lr: 5e-6
  epochs: 100000

data:
  vocab_size: 10000  # BPE/WordPiece
  seq_len: 1024      # Secuencias largas
```

### Recursos Computacionales

| ConfiguraciÃ³n | GPU MÃ­nima | Memoria GPU | Tiempo Entrenamiento |
|---------------|------------|-------------|---------------------|
| **Base** | RTX 3070 | 8GB | ~6 horas |
| **Intermedia** | RTX 4080 | 16GB | ~24 horas |
| **Avanzada** | A100 | 40GB | ~72 horas |

---

## ğŸ—ºï¸ Roadmap de Desarrollo

### Fase 1: Fundamentos (Semanas 1-4)

#### Semana 1: PreparaciÃ³n
- [ ] Configurar entorno de desarrollo
- [ ] Implementar tokenizador de caracteres
- [ ] Crear dataset de traducciÃ³n simple
- [ ] Verificar compatibilidad con HRM base

#### Semana 2: Modelo Base
- [ ] Adaptar configuraciÃ³n HRM para texto
- [ ] Modificar embeddings y heads de salida
- [ ] Implementar funciÃ³n de pÃ©rdida para texto
- [ ] Probar entrenamiento en dataset pequeÃ±o

#### Semana 3: Entrenamiento
- [ ] Entrenar modelo en traducciÃ³n simple
- [ ] Implementar mÃ©tricas de evaluaciÃ³n
- [ ] Optimizar hiperparÃ¡metros
- [ ] Validar convergencia

#### Semana 4: Inferencia
- [ ] Implementar pipeline de inferencia
- [ ] Crear sistema de decodificaciÃ³n
- [ ] Probar generaciÃ³n de texto
- [ ] Documentar resultados iniciales

### Fase 2: OptimizaciÃ³n (Semanas 5-8)

#### Semana 5: TokenizaciÃ³n Avanzada
- [ ] Implementar tokenizador hÃ­brido
- [ ] Comparar rendimiento vs caracteres
- [ ] Optimizar vocabulario
- [ ] Evaluar eficiencia de memoria

#### Semana 6: Arquitectura
- [ ] Experimentar con mÃ¡s capas/ciclos
- [ ] Optimizar mecanismo ACT para texto
- [ ] Probar diferentes configuraciones
- [ ] Analizar trade-offs rendimiento/velocidad

#### Semana 7: Datos y Entrenamiento
- [ ] Expandir datasets de entrenamiento
- [ ] Implementar data augmentation
- [ ] Optimizar pipeline de datos
- [ ] Mejorar estabilidad de entrenamiento

#### Semana 8: EvaluaciÃ³n
- [ ] Implementar mÃ©tricas avanzadas (BLEU, ROUGE)
- [ ] Crear benchmark de evaluaciÃ³n
- [ ] Comparar con baselines
- [ ] Analizar casos de fallo

### Fase 3: Aplicaciones (Semanas 9-12)

#### Semana 9: Casos de Uso
- [ ] Implementar Q&A factual
- [ ] Crear sistema de correcciÃ³n gramatical
- [ ] Probar extracciÃ³n de informaciÃ³n
- [ ] Evaluar en tareas estructuradas

#### Semana 10: Escalabilidad
- [ ] Optimizar para secuencias largas
- [ ] Implementar entrenamiento distribuido
- [ ] Probar en datasets grandes
- [ ] Optimizar memoria y velocidad

#### Semana 11: Robustez
- [ ] Probar en diferentes dominios
- [ ] Evaluar generalizaciÃ³n
- [ ] Implementar manejo de errores
- [ ] Crear tests de regresiÃ³n

#### Semana 12: ProducciÃ³n
- [ ] Crear API de inferencia
- [ ] Implementar sistema de logging
- [ ] Crear documentaciÃ³n de usuario
- [ ] Preparar release inicial

### Fase 4: InvestigaciÃ³n Avanzada (Semanas 13+)

#### InvestigaciÃ³n Continua
- [ ] Explorar generaciÃ³n autorregresiva hÃ­brida
- [ ] Investigar adaptaciÃ³n a idiomas no latinos
- [ ] Experimentar con arquitecturas multimodales
- [ ] Desarrollar tÃ©cnicas de fine-tuning eficiente

#### Optimizaciones Avanzadas
- [ ] Implementar attention sparse para secuencias largas
- [ ] Explorar quantizaciÃ³n y pruning
- [ ] Desarrollar tÃ©cnicas de distillation
- [ ] Investigar arquitecturas neurales evolutivas

---

## ğŸ“Š MÃ©tricas de Ã‰xito

### MÃ©tricas TÃ©cnicas

#### Rendimiento del Modelo
- **Exactitud**: >90% en tareas de traducciÃ³n simple
- **BLEU Score**: >0.7 para traducciones de calidad
- **Convergencia**: <10K pasos para tareas simples
- **Memoria**: <2GB GPU para configuraciÃ³n base

#### Eficiencia de Datos
- **Few-shot**: Aprendizaje efectivo con <5K ejemplos
- **GeneralizaciÃ³n**: >80% exactitud en datos no vistos
- **Transferencia**: AdaptaciÃ³n rÃ¡pida a nuevas tareas

### MÃ©tricas de Negocio

#### Usabilidad
- **Latencia**: <100ms para inferencia en secuencias cortas
- **Throughput**: >100 ejemplos/segundo en batch
- **Escalabilidad**: Soporte para mÃºltiples usuarios concurrentes

#### Calidad
- **Coherencia**: Texto generado coherente y gramaticalmente correcto
- **Relevancia**: Respuestas apropiadas al contexto
- **Diversidad**: Capacidad de generar respuestas variadas

---

## ğŸ”— Referencias y Recursos

### Papers Fundamentales
1. **HRM Original**: "Hierarchical Reasoning Model" - Wang et al. (2025)
2. **Adaptive Computation Time**: "Adaptive Computation Time for Recurrent Neural Networks" - Graves (2016)
3. **Transformer Architecture**: "Attention Is All You Need" - Vaswani et al. (2017)

### Implementaciones de Referencia
- **HRM Base**: [sapientinc/HRM](https://github.com/sapientinc/HRM)
- **FlashAttention**: [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)
- **Transformers**: [huggingface/transformers](https://github.com/huggingface/transformers)

### Datasets Sugeridos
- **TraducciÃ³n**: WMT datasets, OPUS collections
- **Q&A**: SQuAD, Natural Questions
- **CorrecciÃ³n**: CoNLL-2014, JFLEG
- **Resumen**: CNN/DailyMail, XSum

### Herramientas y LibrerÃ­as
```python
# TokenizaciÃ³n
import transformers  # BPE/WordPiece tokenizers
import sentencepiece  # SentencePiece tokenizer

# EvaluaciÃ³n
import nltk  # BLEU, ROUGE scores
import sacrebleu  # Standardized BLEU
import bert_score  # Semantic similarity

# VisualizaciÃ³n
import wandb  # Experiment tracking
import tensorboard  # Training visualization
import matplotlib  # Custom plots
```

---

## ğŸ“ Conclusiones

### Viabilidad TÃ©cnica: âœ… CONFIRMADA

La adaptaciÃ³n de HRM para procesamiento de texto es **tÃ©cnicamente viable** y mantiene las ventajas fundamentales del modelo original:

1. **Eficiencia de Datos**: Potencial para aprendizaje few-shot en tareas de texto
2. **Razonamiento JerÃ¡rquico**: SeparaciÃ³n Ãºtil entre planificaciÃ³n y ejecuciÃ³n
3. **ComputaciÃ³n Adaptativa**: ACT permite procesamiento variable segÃºn complejidad
4. **Estabilidad**: Arquitectura recurrente probada y estable

### Casos de Uso Ã“ptimos

HRLM es especialmente adecuado para:
- âœ… **Tareas estructuradas** con patrones claros
- âœ… **Transformaciones determinÃ­sticas** de texto
- âœ… **Dominios especÃ­ficos** con vocabulario limitado
- âœ… **Aplicaciones few-shot** con datos limitados

### PrÃ³ximos Pasos

1. **Implementar prototipo** siguiendo la guÃ­a de implementaciÃ³n
2. **Validar en tareas simples** como traducciÃ³n bÃ¡sica
3. **Optimizar configuraciÃ³n** para casos de uso especÃ­ficos
4. **Expandir gradualmente** a tareas mÃ¡s complejas
5. **Contribuir al ecosistema** HRM con extensiones de texto

### Impacto Esperado

HRLM tiene el potencial de:
- **Democratizar NLP** con modelos eficientes en datos
- **Acelerar prototipado** de aplicaciones de texto
- **Reducir costos** de entrenamiento para tareas especÃ­ficas
- **Avanzar investigaciÃ³n** en razonamiento jerÃ¡rquico para texto

---

*DocumentaciÃ³n creada para guiar la implementaciÃ³n de Hierarchical Reasoning Language Model (HRLM) basado en la arquitectura HRM original.*

**VersiÃ³n**: 1.0  
**Fecha**: Enero 2025  
**Autor**: AnÃ¡lisis basado en arquitectura HRM de sapientinc  
**Licencia**: Seguir licencia del proyecto HRM original